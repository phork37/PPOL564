{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "569ce366",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "Decision Trees can think of chopping the data into bins via interaction variables.\n",
    "\n",
    "Decision Trees can be a surrogate model, in that it is more interpretable and easier to understand than more complicated models and can be used to inform and interpret models that have higher performance, but are more obtuse.\n",
    "\n",
    "Slides: ericdunford.com/ppol564/lectures/week_12/slides/trees-bagging-and-random-forests.html#5\n",
    "\n",
    "Decision Trees are greedy - make the best decision at the current level and split, regardless of how it impacts future decisions\n",
    "\n",
    "Tree Pruning is one of the main tuning parameter. Tuning is something we can't learn through estimation, but have to specify up front and will change the nature of our prediction. So we try a variety and observe.\n",
    "\n",
    "Shallow vs Deep (number of splits). Deep Trees typically result in overfitting, whereas Shallow trees can lead to underperformance if too small.\n",
    "\n",
    "Work around this by penalizing depth using a \"complexity criterion\" denoted as alpha. To calculate this we add (alpha x T) to the RSS calculation where T is number of terminal nodes and alpha is how much we want to penalize depth.\n",
    "\n",
    "The MSE for train data always gets better as we make the tree larger because we're simply memorizing the data. Instead, we find a balance where cross-validation helps us maximize our OOB results.\n",
    "\n",
    "If a Decision Tree splits on a single variable multiple times, this mimics polynomial relationships and models higher orders of layered interactions.\n",
    "\n",
    "Pros include easy to epxplain, handling qualitative predictors, and can deal with data inconsistencies. Drawbacks are a less predictive accuracy and that it suffers from high variance.\n",
    "\n",
    "In this context, High Variance means that as we add in new data, the structure will change and can change drastically and lead to overfitting. Bias, however, is how tight of a fit the model is to the data. There's an inherent trade off of Bias and Variance.\n",
    "\n",
    "\n",
    "## Classification Trees\n",
    "\n",
    "Categorical rather than continuous outcome.\n",
    "Predicts the most commonly occuring class of training observiations in the region to which it belongs.\n",
    "\n",
    "Gini index is a measurement of error that gets small if node purity is high.\n",
    "\n",
    "Formula\n",
    "G = sum( P_hat_mk(1-P_hat_mk) )\n",
    "where the sum is 1 to k and we're considering feature m\n",
    "p(1-p) = a bivarate variance we're trying to minimize\n",
    "and p_hat is the outcome we guess based on the tree result\n",
    "\n",
    "\n",
    "## Bagging\n",
    "Bootstrap aggregation (Bagging) is a general purpose procedure for reducing the variance of a stat learning model.\n",
    "\"Randomly looking at different vantage points of the data\"\n",
    "\n",
    "The idea is to \n",
    "- take many training sets from the data (subsets with replacement)\n",
    "- make separate trees on each training set\n",
    "- average across the predictions from each tree\n",
    "\n",
    "\n",
    "See slides for function that captures the mathematical depiction of taking b training sets and creating a tree for each one and then averaging the prediction from each tree.\n",
    "\n",
    "These bagged models, however, are highly correlated with one another because whatever the most predictive single variable is will serve as the first split and make trees similar.\n",
    "\n",
    "However, if we randomly select columns as well, each tree will be less correlated. \"Gives other variables a fighting chance.\"\n",
    "\n",
    "Number of predictors (columns) is a tuning parameter (max_features)\n",
    "\n",
    "The other thing it allows us to do is have more parameters than data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
